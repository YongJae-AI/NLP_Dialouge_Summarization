{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama 3 8B (base) + LoRA / QLoRA (Colabìš©)\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ Google Colabì—ì„œ **meta-llama/Meta-Llama-3-8B (base)**ë¥¼ ëŒ€ìƒìœ¼ë¡œ\n",
    "- LoRA (fp16)\n",
    "- QLoRA (4bit, nf4)\n",
    "ë¥¼ ì‚¬ìš©í•´ í•œêµ­ì–´ ëŒ€í™” ìš”ì•½ì„ í•™ìŠµ/í‰ê°€/ì œì¶œìš© CSV ìƒì„±ê¹Œì§€ ìˆ˜í–‰í•˜ëŠ” ì½”ë“œì…ë‹ˆë‹¤.\n",
    "\n",
    "âš ï¸ ì¤€ë¹„ ì‚¬í•­\n",
    "- Hugging Face ê³„ì •ì—ì„œ `meta-llama/Meta-Llama-3-8B` ëª¨ë¸ ì•¡ì„¸ìŠ¤ í—ˆìš©\n",
    "- HF Access Token ì¤€ë¹„ (read ê¶Œí•œ)\n",
    "- Colab GPU ëŸ°íƒ€ì„ (T4 / V100 / A100 ë“±)\n",
    "\n",
    "ğŸ“‚ ë°ì´í„°\n",
    "- `data/train.csv`, `data/dev.csv`, `data/test.csv` í˜•ì‹ì˜ íŒŒì¼ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "- Colab Files íŒ¨ë„ ë˜ëŠ” `files.upload()`ë¡œ ì—…ë¡œë“œí•œ ë’¤, `data/` í´ë” ì•ˆìœ¼ë¡œ ì˜®ê²¨ ì‚¬ìš©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1. í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
    "\n",
    "!pip install -q \\\n",
    "  \"torch>=2.1\" \\\n",
    "  \"transformers>=4.40.0\" \\\n",
    "  \"peft>=0.10.0\" \\\n",
    "  \"accelerate>=0.30.0\" \\\n",
    "  \"bitsandbytes>=0.43.0\" \\\n",
    "  \"datasets>=2.19.0\" \\\n",
    "  \"evaluate>=0.4.0\" \\\n",
    "  \"pandas\" \\\n",
    "  \"tqdm\""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 2. GPU í™•ì¸ + Hugging Face ë¡œê·¸ì¸\n",
    "\n",
    "import torch, os\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    !nvidia-smi\n",
    "except Exception:\n",
    "    print(\"Colabì´ ì•„ë‹ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. GPU ìƒíƒœëŠ” ìœ„ ì¶œë ¥ë§Œ ì°¸ê³ í•˜ì„¸ìš”.\")\n",
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "print(\"\\n*** Hugging Face í† í°ì„ ì…ë ¥í•˜ì„¸ìš” (í•œ ë²ˆë§Œ) ***\")\n",
    "hf_token = input(\"Enter your Hugging Face token: \").strip()\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 3. ë°ì´í„° ì—…ë¡œë“œ ë° ë¡œë”©\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "DATA_DIR = Path(\"data\")\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# ë£¨íŠ¸ì— ì˜¬ë¼ì˜¨ train/dev/test.csv ë¥¼ data/ë¡œ ì´ë™ (ì´ë¯¸ data/ì— ì˜¬ë ¸ë‹¤ë©´ ê±´ë„ˆëœ€)\n",
    "for name in [\"train.csv\", \"dev.csv\", \"test.csv\"]:\n",
    "    if Path(name).exists() and not (DATA_DIR / name).exists():\n",
    "        os.replace(name, DATA_DIR / name)\n",
    "\n",
    "train_path = DATA_DIR / \"train.csv\"\n",
    "dev_path   = DATA_DIR / \"dev.csv\"\n",
    "test_path  = DATA_DIR / \"test.csv\"\n",
    "\n",
    "assert train_path.exists(), f\"íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {train_path}\"\n",
    "assert dev_path.exists(),   f\"íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {dev_path}\"\n",
    "assert test_path.exists(),  f\"íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {test_path}\"\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "dev_df   = pd.read_csv(dev_path)\n",
    "test_df  = pd.read_csv(test_path)\n",
    "\n",
    "train_df.head(), dev_df.head(), test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 4. í”„ë¡¬í”„íŠ¸ ìƒì„± í•¨ìˆ˜ + HF Dataset ë³€í™˜\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "def build_prompt(dialogue: str, summary: str | None = None) -> str:\n",
    "    \"\"\"Llama 3 baseìš© Instruction-style í”„ë¡¬í”„íŠ¸.\"\"\"\n",
    "    system = (\n",
    "        \"ë‹¹ì‹ ì€ í•œêµ­ì–´ ëŒ€í™” ìš”ì•½ ë¹„ì„œì´ë‹¤.\\n\"\n",
    "        \"ëŒ€í™”ë¥¼ ì½ê³ , í•œë‘ ë¬¸ì¥ìœ¼ë¡œ í•µì‹¬ ë‚´ìš©ì„ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ë¼.\\n\"\n",
    "    )\n",
    "    user = f\"ìš”ì•½: ëŒ€í™”ì˜ í•µì‹¬ë§Œ ê°„ê²°í•˜ê²Œ í•œë‘ ë¬¸ì¥ìœ¼ë¡œ ì •ë¦¬í•˜ì‹œì˜¤.\\n\\n{dialogue.strip()}\\n\"\n",
    "    if summary is None:\n",
    "        assistant = \"\"\n",
    "    else:\n",
    "        assistant = summary.strip()\n",
    "    return f\"<|system|>\\n{system}\\n<|user|>\\n{user}\\n<|assistant|>\\n{assistant}\"\n",
    "\n",
    "def make_sft_dataset(df: pd.DataFrame) -> Dataset:\n",
    "    texts = [build_prompt(row[\"dialogue\"], row[\"summary\"]) for _, row in df.iterrows()]\n",
    "    return Dataset.from_dict({\"text\": texts})\n",
    "\n",
    "def make_infer_dataset(df: pd.DataFrame) -> Dataset:\n",
    "    prompts = [build_prompt(row[\"dialogue\"], None) for _, row in df.iterrows()]\n",
    "    fnames  = df[\"fname\"].tolist()\n",
    "    return Dataset.from_dict({\"prompt\": prompts, \"fname\": fnames})\n",
    "\n",
    "train_dataset = make_sft_dataset(train_df)\n",
    "dev_dataset   = make_sft_dataset(dev_df)\n",
    "test_dataset  = make_infer_dataset(test_df)\n",
    "\n",
    "train_dataset[:2], dev_dataset[:2], test_dataset[:2]"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 5. Llama 3 8B base ë¡œë“œ + LoRA / QLoRA ì„¸íŒ…\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "MODEL_NAME = \"meta-llama/Meta-Llama-3-8B\"  # base, ê·œì¹™ ì¤€ìˆ˜ ë°±ë³¸\n",
    "USE_4BIT = True  # True: QLoRA(4bit), False: fp16+LoRA\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "if USE_4BIT:\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "    )\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",\n",
        "    )\n",
        "    model = prepare_model_for_kbit_training(model)\n",
        "else:\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\",\n",
        "    )\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=64,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.config.use_cache = False  # gradient_checkpointingê³¼ ì¶©ëŒ ë°©ì§€\n",
        "\n",
        "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 6. í† í¬ë‚˜ì´ì¦ˆ + Trainer ì„¸íŒ…\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling, TrainingArguments, Trainer\n",
    "\n",
    "max_input_length = 1280 if USE_4BIT else 1024\n",
    "max_new_tokens = 80\n",
    "per_device_batch_size = 1\n",
    "grad_accum_steps = 8\n",
    "num_train_epochs = 3\n",
    "learning_rate = 2e-4\n",
    "\n",
    "def tokenize_sft(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        max_length=max_input_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "train_tokenized = train_dataset.map(tokenize_sft, batched=True, remove_columns=[\"text\"])\n",
    "dev_tokenized   = dev_dataset.map(tokenize_sft,   batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "def add_labels(batch):\n",
    "    batch[\"labels\"] = batch[\"input_ids\"].copy()\n",
    "    return batch\n",
    "\n",
    "train_tokenized = train_tokenized.map(add_labels, batched=True)\n",
    "dev_tokenized   = dev_tokenized.map(add_labels,   batched=True)\n",
    "\n",
    "output_dir = \"llama3_lora_ckpt\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=per_device_batch_size,\n",
    "    per_device_eval_batch_size=per_device_batch_size,\n",
    "    gradient_accumulation_steps=grad_accum_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.05,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    "    fp16=not USE_4BIT,\n",
    "    gradient_checkpointing=True,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=[],\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized,\n",
    "    eval_dataset=dev_tokenized,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 7. í•™ìŠµ ì‹œì‘\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 8. dev ROUGE ì¸¡ì • (ì„ íƒ)\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def generate_summary(prompt: str) -> str:\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_input_length,\n",
    "    ).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        gen_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            num_beams=4,\n",
    "        )\n",
    "    gen_text = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
    "    if \"<|assistant|>\" in gen_text:\n",
    "        return gen_text.split(\"<|assistant|>\")[-1].strip()\n",
    "    return gen_text.strip()\n",
    "\n",
    "preds = []\n",
    "refs  = []\n",
    "\n",
    "for _, row in dev_df.iterrows():\n",
    "    prompt = build_prompt(row[\"dialogue\"], None)\n",
    "    pred = generate_summary(prompt)\n",
    "    preds.append(pred)\n",
    "    refs.append(row[\"summary\"])\n",
    "\n",
    "result = rouge.compute(predictions=preds, references=refs)\n",
    "result = {k: round(v * 100, 2) for k, v in result.items()}\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 9. test inference + ì œì¶œìš© CSV ì €ì¥\n",
    "\n",
    "summaries = []\n",
    "for _, row in test_df.iterrows():\n",
    "    prompt = build_prompt(row[\"dialogue\"], None)\n",
    "    fname  = row[\"fname\"]\n",
    "    summary = generate_summary(prompt)\n",
    "    summaries.append({\"fname\": fname, \"summary\": summary})\n",
    "\n",
    "pred_df = pd.DataFrame(summaries)\n",
    "pred_df.to_csv(\"llama3_lora_test.csv\", index=False)\n",
    "pred_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

