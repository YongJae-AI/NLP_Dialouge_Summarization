{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f9ec82e",
   "metadata": {},
   "source": [
    "# Llama 3 8B (base) + LoRA / QLoRA (Colab용)\n",
    "\n",
    "이 노트북은 Google Colab에서 **meta-llama/Meta-Llama-3-8B (base)**를 대상으로\n",
    "- LoRA (fp16)\n",
    "- QLoRA (4bit, nf4)\n",
    "를 사용해 한국어 대화 요약을 학습/평가/제출용 CSV 생성까지 수행하는 코드입니다.\n",
    "\n",
    "⚠️ 준비 사항\n",
    "- Hugging Face 계정에서 `meta-llama/Meta-Llama-3-8B` 모델 액세스 허용 (이미 완료)\n",
    "- HF Access Token 준비 (read 권한) (이미 완료)\n",
    "- Google Drive에 데이터 저장 (`llama_summarization/data` 폴더)\n",
    "- Colab GPU 런타임 (T4 / V100 / A100 등)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642d57a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 필수 패키지 설치\n",
    "\n",
    "!pip install -q \\\n",
    "  \"torch>=2.1\" \\\n",
    "  \"transformers>=4.40.0\" \\\n",
    "  \"peft>=0.10.0\" \\\n",
    "  \"accelerate>=0.30.0\" \\\n",
    "  \"bitsandbytes>=0.43.0\" \\\n",
    "  \"datasets>=2.19.0\" \\\n",
    "  \"evaluate>=0.4.0\" \\\n",
    "  \"pandas\" \\\n",
    "  \"tqdm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bfff70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. GPU 확인 + Google Drive 마운트 + Hugging Face 로그인\n",
    "\n",
    "import torch, os\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    # ⭐️ Google Drive 마운트\n",
    "    drive.mount('/content/drive')\n",
    "    !nvidia-smi\n",
    "except Exception:\n",
    "    print(\"Colab이 아닐 수도 있습니다. GPU 상태는 위 출력만 참고하세요.\")\n",
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "print(\"\\n*** Hugging Face 토큰을 입력하세요 (한 번만) ***\")\n",
    "# 토큰을 직접 붙여넣고 Enter\n",
    "hf_token = input(\"Enter your Hugging Face token: \").strip()\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aeca9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Google Drive에서 데이터 로딩\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ⭐️ 사용자 지정 Google Drive 경로 수정\n",
    "# 파일(train.csv, dev.csv, test.csv)이 MyDrive 바로 아래에 있는 경우\n",
    "ROOT_DIR = Path(\"/content/drive/MyDrive\")\n",
    "\n",
    "# 파일 경로를 ROOT_DIR 바로 아래로 설정\n",
    "train_path = ROOT_DIR / \"train.csv\"\n",
    "dev_path   = ROOT_DIR / \"dev.csv\"\n",
    "test_path  = ROOT_DIR / \"test.csv\"\n",
    "\n",
    "# 파일이 존재하는지 확인\n",
    "assert train_path.exists(), f\"파일이 없습니다: {train_path}\"\n",
    "assert dev_path.exists(),   f\"파일이 없습니다: {dev_path}\"\n",
    "assert test_path.exists(),  f\"파일이 없습니다: {test_path}\"\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "dev_df   = pd.read_csv(dev_path)\n",
    "test_df  = pd.read_csv(test_path)\n",
    "\n",
    "print(f\"train/dev/test 데이터 크기: {len(train_df)}/{len(dev_df)}/{len(test_df)}\")\n",
    "train_df.head(), dev_df.head(), test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4e3e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 프롬프트 생성 함수 + HF Dataset 변환\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "def build_prompt(dialogue: str, summary: str | None = None) -> str:\n",
    "    \"\"\"Llama 3 base용 Instruction-style 프롬프트.\"\"\"\n",
    "    system = (\n",
    "        \"당신은 한국어 대화 요약 비서이다.\\n\"\n",
    "        \"대화를 읽고, 한두 문장으로 핵심 내용을 간결하게 요약하라.\\n\"\n",
    "    )\n",
    "    user = f\"요약: 대화의 핵심만 간결하게 한두 문장으로 정리하시오.\\n\\n{dialogue.strip()}\\n\"\n",
    "    if summary is None:\n",
    "        assistant = \"\"\n",
    "    else:\n",
    "        assistant = summary.strip()\n",
    "    return f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{system}\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{user}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{assistant}\"\n",
    "\n",
    "def make_sft_dataset(df: pd.DataFrame) -> Dataset:\n",
    "    texts = [build_prompt(row[\"dialogue\"], row[\"summary\"]) for _, row in df.iterrows()]\n",
    "    return Dataset.from_dict({\"text\": texts})\n",
    "\n",
    "def make_infer_dataset(df: pd.DataFrame) -> Dataset:\n",
    "    # 추론 시에는 summary를 포함하지 않습니다.\n",
    "    prompts = [build_prompt(row[\"dialogue\"], None) for _, row in df.iterrows()]\n",
    "    fnames  = df[\"fname\"].tolist() if \"fname\" in df.columns else list(range(len(df)))\n",
    "    return Dataset.from_dict({\"prompt\": prompts, \"fname\": fnames})\n",
    "\n",
    "train_dataset = make_sft_dataset(train_df)\n",
    "dev_dataset   = make_sft_dataset(dev_df)\n",
    "test_dataset  = make_infer_dataset(test_df)\n",
    "\n",
    "print(\"\\n--- Train Dataset Example ---\")\n",
    "print(train_dataset[0][\"text\"])\n",
    "\n",
    "train_dataset[:2], dev_dataset[:2], test_dataset[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138e5656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Llama 3 8B base 로드 + LoRA / QLoRA 세팅\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "MODEL_NAME = \"meta-llama/Meta-Llama-3-8B\"  # base 모델 사용\n",
    "USE_4BIT = True  # True: QLoRA(4bit) - Colab T4에 필수\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "if USE_4BIT:\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16, # T4는 BF16 지원X, FP16으로 자동 전환됨\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.config.use_cache = False  # gradient_checkpointing과 충돌 방지\n",
    "model.config.pretraining_tp = 1 # 분산 학습 최적화 설정\n",
    "\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b33d00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 토크나이즈 + Trainer 세팅\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling, TrainingArguments, Trainer\n",
    "\n",
    "max_input_length = 1280 \n",
    "max_new_tokens = 80 # 생성할 요약문의 최대 길이\n",
    "per_device_batch_size = 1 # VRAM 16GB 한계로 1 고정 (⭐)\n",
    "grad_accum_steps = 8 # 배치 크기 8의 효과를 내기 위한 경사 누적 단계 (⭐)\n",
    "num_train_epochs = 3\n",
    "learning_rate = 2e-4\n",
    "\n",
    "def tokenize_sft(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        max_length=max_input_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "train_tokenized = train_dataset.map(tokenize_sft, batched=True, remove_columns=[\"text\"])\n",
    "dev_tokenized   = dev_dataset.map(tokenize_sft,   batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "def add_labels(batch):\n",
    "    # Causal LM 학습을 위해 input_ids를 labels로 사용\n",
    "    batch[\"labels\"] = batch[\"input_ids\"].copy()\n",
    "    return batch\n",
    "\n",
    "train_tokenized = train_tokenized.map(add_labels, batched=True)\n",
    "dev_tokenized   = dev_tokenized.map(add_labels,   batched=True)\n",
    "\n",
    "output_dir = \"llama3_lora_ckpt\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=per_device_batch_size,\n",
    "    per_device_eval_batch_size=per_device_batch_size,\n",
    "    gradient_accumulation_steps=grad_accum_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.05,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    "    fp16=not USE_4BIT,\n",
    "    gradient_checkpointing=True, # 메모리 절약 필수 (⭐)\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=[],\n",
    "    optim=\"paged_adamw_8bit\", # 메모리 효율적인 옵티마이저\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized,\n",
    "    eval_dataset=dev_tokenized,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e975d5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. 학습 시작\n",
    "\n",
    "trainer.train()\n",
    "# LoRA 어댑터만 저장\n",
    "trainer.model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29a5aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. dev ROUGE 측정 (선택)\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def generate_summary(prompt: str) -> str:\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_input_length,\n",
    "    ).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        # max_new_tokens만큼만 새로운 텍스트를 생성하도록 제한\n",
    "        gen_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            num_beams=1, # Colab 환경을 고려하여 빔 서치(Beam Search) 1로 설정하여 속도/메모리 최적화\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # 입력 프롬프트 부분을 제거하고 생성된 텍스트만 추출\n",
    "    # inputs의 길이는 토큰 개수\n",
    "    gen_text = tokenizer.decode(gen_ids[0][len(inputs['input_ids'][0]):], skip_special_tokens=True)\n",
    "    \n",
    "    # 출력에서 Llama 3 프롬프트 형식의 특수 토큰(<|eot_id|>, <|assistant|>, <|end_of_text|>) 제거\n",
    "    cleaned_text = gen_text.split('<|eot_id|>')[0].strip()\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "preds = []\n",
    "refs  = []\n",
    "\n",
    "# dev set 전체를 추론하기에는 시간이 오래 걸리므로, 샘플 100개만 사용하거나 건너뛰는 것을 고려하세요.\n",
    "print(\"Dev Set 추론 시작...\")\n",
    "\n",
    "for i, row in dev_df.iterrows():\n",
    "    if i >= 100: break # 빠른 테스트를 위해 100개만 실행\n",
    "    prompt = build_prompt(row[\"dialogue\"], None)\n",
    "    pred = generate_summary(prompt)\n",
    "    preds.append(pred)\n",
    "    refs.append(row[\"summary\"])\n",
    "\n",
    "result = rouge.compute(predictions=preds, references=refs)\n",
    "result = {k: round(v * 100, 2) for k, v in result.items()}\n",
    "print(\"\\n--- ROUGE 결과 (상위 100개) ---\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeaf0122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. test inference + 제출용 CSV 저장\n",
    "\n",
    "summaries = []\n",
    "for _, row in test_df.iterrows():\n",
    "    prompt = build_prompt(row[\"dialogue\"], None)\n",
    "    fname  = row[\"fname\"] if \"fname\" in row else f\"test_{i}\"\n",
    "    summary = generate_summary(prompt)\n",
    "    summaries.append({\"fname\": fname, \"summary\": summary})\n",
    "\n",
    "pred_df = pd.DataFrame(summaries)\n",
    "pred_df.to_csv(\"llama3_lora_test.csv\", index=False)\n",
    "print(\"\\n--- 제출 파일 저장 완료 (llama3_lora_test.csv) ---\")\n",
    "pred_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
